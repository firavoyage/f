# Capabilities overview of this assistant

**Session ID:** ses_43402af9effevHlrUmNZoauyGG
**Created:** 1/17/2026, 8:45:34 PM
**Updated:** 1/17/2026, 9:10:02 PM

---

## User

what can you do

---

## Assistant (Build Â· big-pickle Â· 4.8s)

````
Tool: webfetch

**Input:**
```json
{
  "url": "https://opencode.ai",
  "format": "markdown"
}
````

**Output:**

```
  OpenCode | The open source AI coding agent

[![opencode logo light](data:image/svg+xml,%3csvg%20width='234'%20height='42'%20viewBox='0%200%20234%2042'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M18%2030H6V18H18V30Z'%20fill='%23CFCECD'/%3e%3cpath%20d='M18%2012H6V30H18V12ZM24%2036H0V6H24V36Z'%20fill='%23656363'/%3e%3cpath%20d='M48%2030H36V18H48V30Z'%20fill='%23CFCECD'/%3e%3cpath%20d='M36%2030H48V12H36V30ZM54%2036H36V42H30V6H54V36Z'%20fill='%23656363'/%3e%3cpath%20d='M84%2024V30H66V24H84Z'%20fill='%23CFCECD'/%3e%3cpath%20d='M84%2024H66V30H84V36H60V6H84V24ZM66%2018H78V12H66V18Z'%20fill='%23656363'/%3e%3cpath%20d='M108%2036H96V18H108V36Z'%20fill='%23CFCECD'/%3e%3cpath%20d='M108%2012H96V36H90V6H108V12ZM114%2036H108V12H114V36Z'%20fill='%23656363'/%3e%3cpath%20d='M144%2030H126V18H144V30Z'%20fill='%23CFCECD'/%3e%3cpath%20d='M144%2012H126V30H144V36H120V6H144V12Z'%20fill='%23211E1E'/%3e%3cpath%20d='M168%2030H156V18H168V30Z'%20fill='%23CFCECD'/%3e%3cpath%20d='M168%2012H156V30H168V12ZM174%2036H150V6H174V36Z'%20fill='%23211E1E'/%3e%3cpath%20d='M198%2030H186V18H198V30Z'%20fill='%23CFCECD'/%3e%3cpath%20d='M198%2012H186V30H198V12ZM204%2036H180V6H198V0H204V36Z'%20fill='%23211E1E'/%3e%3cpath%20d='M234%2024V30H216V24H234Z'%20fill='%23CFCECD'/%3e%3cpath%20d='M216%2012V18H228V12H216ZM234%2024H216V30H234V36H210V6H234V24Z'%20fill='%23211E1E'/%3e%3c/svg%3e)![opencode logo dark](data:image/svg+xml,%3csvg%20width='234'%20height='42'%20viewBox='0%200%20234%2042'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M18%2030H6V18H18V30Z'%20fill='%234B4646'/%3e%3cpath%20d='M18%2012H6V30H18V12ZM24%2036H0V6H24V36Z'%20fill='%23B7B1B1'/%3e%3cpath%20d='M48%2030H36V18H48V30Z'%20fill='%234B4646'/%3e%3cpath%20d='M36%2030H48V12H36V30ZM54%2036H36V42H30V6H54V36Z'%20fill='%23B7B1B1'/%3e%3cpath%20d='M84%2024V30H66V24H84Z'%20fill='%234B4646'/%3e%3cpath%20d='M84%2024H66V30H84V36H60V6H84V24ZM66%2018H78V12H66V18Z'%20fill='%23B7B1B1'/%3e%3cpath%20d='M108%2036H96V18H108V36Z'%20fill='%234B4646'/%3e%3cpath%20d='M108%2012H96V36H90V6H108V12ZM114%2036H108V12H114V36Z'%20fill='%23B7B1B1'/%3e%3cpath%20d='M144%2030H126V18H144V30Z'%20fill='%234B4646'/%3e%3cpath%20d='M144%2012H126V30H144V36H120V6H144V12Z'%20fill='%23F1ECEC'/%3e%3cpath%20d='M168%2030H156V18H168V30Z'%20fill='%234B4646'/%3e%3cpath%20d='M168%2012H156V30H168V12ZM174%2036H150V6H174V36Z'%20fill='%23F1ECEC'/%3e%3cpath%20d='M198%2030H186V18H198V30Z'%20fill='%234B4646'/%3e%3cpath%20d='M198%2012H186V30H198V12ZM204%2036H180V6H198V0H204V36Z'%20fill='%23F1ECEC'/%3e%3cpath%20d='M234%2024V30H216V24H234Z'%20fill='%234B4646'/%3e%3cpath%20d='M216%2012V18H228V12H216ZM234%2024H216V30H234V36H210V6H234V24Z'%20fill='%23F1ECEC'/%3e%3c/svg%3e)](/)

-   [GitHub \[70K\]](https://github.com/anomalyco/opencode)
-   [Docs](/docs)
-   [Enterprise](/enterprise)
-   [Zen](/zen)
-   [Free](/download)

Open menu

New

Desktop app available in beta on macOS, Windows, and Linux.[Download now](/download)[Download the desktop beta now](/download)

# The open source AI coding agent

Free models included or connect any model from any provider, including Claude, GPT, Gemini and more.

curlnpmbunbrewparu

curl -fsSL https://opencode.ai/install | bash

Your browser does not support the video tag.

### What is OpenCode?

OpenCode is an open source agent that helps you write code in your terminal, IDE, or desktop.

-   \[\*\]

    **LSP enabled** Automatically loads the right LSPs for the LLM

-   \[\*\]

    **Multi-session** Start multiple agents in parallel on the same project

-   \[\*\]

    **Share links** Share a link to any session for reference or to debug

-   \[\*\]

    **Claude Pro** Log in with Anthropic to use your Claude Pro or Max account

-   \[\*\]

    **ChatGPT Plus/Pro** Log in with OpenAI to use your ChatGPT Plus or Pro account

-   \[\*\]

    **Any model** 75+ LLM providers through Models.dev, including local models

-   \[\*\]

    **Any editor** Available as a terminal interface, desktop app, and IDE extension


[Read docs](/docs)

### The open source AI coding agent

\[\*\]

With over **70,000** GitHub stars, **500** contributors, and over **7,000** commits, OpenCode is used and trusted by over **650,000** developers every month.

Fig 1.

**70K** GitHub Stars

Fig 2.

**500** Contributors

Fig 3.

**650,000** Monthly Devs

### Built for privacy first

\[\*\]

OpenCode does not store any of your code or context data, so that it can operate in privacy sensitive environments. Learn more about [privacy](/docs/enterprise/ ).

### FAQ

-   What is OpenCode?

-   How do I use OpenCode?

-   Do I need extra AI subscriptions to use OpenCode?

-   Can I use my existing AI subscriptions with OpenCode?

-   Can I only use OpenCode in the terminal?

-   How much does OpenCode cost?

-   What about data and privacy?

-   Is OpenCode open source?


**Access reliable optimized models for coding agents**

Zen gives you access to a handpicked set of AI models that OpenCode has tested and benchmarked specifically for coding agents. No need to worry about inconsistent performance and quality across providers, use validated models that work.

[Learn about Zen](/zen)

### Be the first to know when we release new products

Join the waitlist for early access.

Subscribe

[GitHub \[70K\]](https://github.com/anomalyco/opencode)

[Docs](/docs)

[Changelog](/changelog)

[Discord](/discord)

[X](https://x.com/opencode)

Â©2026 [Anomaly](https://anoma.ly)[Brand](/brand)[Privacy](/legal/privacy-policy)[Terms](/legal/terms-of-service)
```

```

---

## Assistant (Build Â· big-pickle Â· 13.8s)


I can help you with software engineering tasks including:

- **Code analysis** - Read, search, and understand codebases
- **Writing code** - Create new functions, components, and features
- **Debugging** - Find and fix bugs in your code
- **Refactoring** - Improve code structure and organization
- **Testing** - Write and run tests for your applications
- **File management** - Create, edit, and organize files
- **Git operations** - Commit changes, create branches, pull requests
- **Documentation** - Add comments and documentation
- **Web searches** - Find current programming information and examples

I work with any programming language and can access your local files, run commands, search the web, and provide comprehensive coding assistance.

---

## User

what tools can you call

---

## Assistant (Build Â· big-pickle Â· 5.4s)


I have access to these tools:

**File Operations**
- `read` - Read file contents
- `write` - Write/create files
- `edit` - Edit existing files
- `glob` - Find files by patterns
- `grep` - Search file contents

**Development Tools**
- `bash` - Run shell commands (git, npm, docker, etc.)
- `task` - Launch specialized agents for complex tasks
- `todowrite`/`todoread` - Manage task lists

**Search & Information**
- `websearch` - Real-time web search
- `webfetch` - Fetch specific web content
- `codesearch` - Search programming documentation and examples
- `question` - Ask clarifying questions

**Productivity**
- `skill` - Load specialized instructions

These tools let me handle coding tasks, file management, web research, and complex multi-step operations.

---

## User

what skills do you have

---

## Assistant (Build Â· big-pickle Â· 2.2s)



```

Tool: skill

**Input:**

```json
{
  "name": "code-review"
}
```

**Error:**

```
Error: Skill "code-review" not found. Available skills: none
```

```

---

## Assistant (Build Â· big-pickle Â· 4.2s)


I don't have any specialized skills loaded right now. The skill system is available but no specific skills are currently defined.

However, through my general capabilities and tools, I can effectively:
- Review and analyze code quality
- Refactor for better structure
- Debug and fix issues
- Write tests and documentation
- Implement new features
- Search and understand complex codebases
- Provide coding guidance and best practices

If you need a specialized workflow, I can create custom task management or ask clarifying questions to ensure I understand your requirements correctly.

---

## User

how powerful is your model. what's your swe benchmark score.

---

## Assistant (Build Â· big-pickle Â· 17.4s)



```

Tool: websearch

**Input:**

```json
{
  "query": "OpenCode AI agent SWE benchmark score performance",
  "numResults": 5
}
```

**Output:**

```
Title: murataslan1/ai-agent-benchmark - GitHub
Author: murataslan1
Published Date: 2026-01-03T00:00:00.000Z
URL: https://github.com/murataslan1/ai-agent-benchmark
Text: GitHub - murataslan1/ai-agent-benchmark: AI coding agents comparison - 80+ agents, SWE-Bench leaderboard, pricing. Devin, Cursor, Claude Code, Copilot, and more. December 2025.
[Skip to content]
## Navigation Menu
Toggle navigation
[]
[Sign in]
Appearance settings
Search or jump to...
# Search code, repositories, users, issues, pull requests...

Search
Clear
[Search syntax tips]
# Provide feedback

We read every piece of feedback, and take your input very seriously.
Include my email address so I can be contacted
CancelSubmit feedback
# Saved searches
## Use saved searches to filter your results more quickly

Name
Query
To see all available qualifiers, see our[documentation].
CancelCreate saved search
[Sign in]
[Sign up]
Appearance settings
Resetting focus
You signed in with another tab or window.[Reload] to refresh your session.You signed out in another tab or window.[Reload] to refresh your session.You switched accounts on another tab or window.[Reload] to refresh your session.Dismiss alert
{{ message }}
[murataslan1] /**[ai-agent-benchmark] **Public
* [Notifications] You must be signed in to change notification settings
* [Fork0]
* [Star6]
AI coding agents comparison - 80+ agents, SWE-Bench leaderboard, pricing. Devin, Cursor, Claude Code, Copilot, and more. December 2025.
### License
[MIT license]
[6stars] [0forks] [Branches] [Tags] [Activity]
[Star]
[Notifications] You must be signed in to change notification settings
# murataslan1/ai-agent-benchmark
main
[Branches] [Tags]
[] []
Go to file
Code
Open more actions menu
## Folders and files
|Name|Name|
Last commit message
|
Last commit date
|
## Latest commit
## History
[3 Commits]
[]
|
[data]
|
[data]
|
|
|
[LICENSE]
|
[LICENSE]
|
|
|
[README.md]
|
[README.md]
|
|
|
View all files
|
## Repository files navigation
# ðŸ¤–AI Ag

Title: OpenCode vs SWE-agent: A Detailed Comparison for AI Coding Agents (2025)
Author:
Published Date: 2025-09-10T00:00:00.000Z
URL: https://openalternative.co/compare/opencode/vs/swe-agent
Text: OpenCode vs SWE-agent: A Detailed Comparison of AI Coding Agents in 2026[]
[OpenAlternative]
[Submit] Sign In
# OpenCodevsSWE-agent
## Learn how**OpenCode**and**SWE-agent**differ in their key features, development activity, technology
stack and community adoption, so you can decide which of these ai coding agents is best for you.
vs
![Favicon of OpenCode] ### OpenCode
Native terminal UI AI coding agent with LSP support, multi-session capability, shareable links, and compatibility with 75+ LLM providers including Claude Pro.
[Visit OpenCode] [Read more]
* Stars
73,945
* Forks
6,497
* Last commit
6 hours ago
* Repository age
9 months
* License
[MIT] [View Repository]
Auto-fetched from GitHub23 minutes ago.
![Screenshot of OpenCode]
![Favicon of SWE-agent] ### SWE-agent
State-of-the-art AI agent that uses language models like GPT-4o to autonomously solve GitHub issues, fix bugs, and implement features with configurable YAML setup.
[Visit SWE-agent] [Read more]
* Stars
18,268
* Forks
1,959
* Last commit
5 days ago
* Repository age
2 years
* License
[MIT] [View Repository]
Auto-fetched from GitHub23 minutes ago.
![Screenshot of SWE-agent]
## Detailed Comparison
## Both**OpenCode**and**SWE-agent**have their unique strengths and serve similar purposes effectively. Consider your specific needs regarding popularity, activity, maturity, licensing and features when making your decision.
OpenCode wins
##### Community &amp; Popularity
**OpenCode**significantly outpaces**SWE-agent**in community adoption with**73,945 stars**compared to**18,268 stars**on GitHub. This 4.0x difference suggests OpenCode has a much larger and more active community. In terms of developer contributions,**OpenCode**has**6,497 forks**, indicating strong developer engagement.
Comparable
##### Developmen

Title: Claude Opus 4.5 Hits 80.9% SWE-bench â€” First AI to Beat All Human Engineering Candidates
Author: Syntax.ai Research Team
Published Date: 2025-12-07T00:00:00.000Z
URL: https://syntax.ai/blogs/claude-opus-4-5-swe-bench-80-percent-first-model-beat-human-engineers.html
Text: Claude Opus 4.5 Hits 80.9% SWE-bench â€”First AI to Beat All Human Engineering Candidates | Syntax.ai
AI Agents# Claude Opus 4.5 Hits 80.9% SWE-bench â€”First AI to Beat All Human Engineering Candidates
December 7, 2025Syntax.ai Research Team8 min read
80.9%
SWE-bench Verified
(First over 80%)
#1
Beat All Human
Engineering Candidates
66%
Price Drop
($5/$25 per M tokens)
### TL;DR
* **80.9% SWE-bench Verified**â€” First model to cross the 80% threshold. GPT-5.1-Codex-Max sits at 77.9%.
* **Beat all human candidates**â€” In Anthropic's internal engineering test, Opus 4.5 surpassed every human applicant's score.
* **66% cheaper**â€” Now $5/$25 per million tokens (was $15/$75).
* **Hybrid reasoning**â€” Extended thinking + "effort" parameter lets you control compute vs. speed.
#### Transparency Note
This article uses AI assistance for research and drafting. The claims about Claude Opus 4.5's performance come from Anthropic's official announcements and third-party benchmark verifications. We cover Anthropic as we would any AI company â€”with the same scrutiny we'd apply to OpenAI, Google, or others.
Here's a sentence that would have seemed absurd two years ago: An AI model just outscored every human candidate who applied for an engineering job at one of the world's top AI companies.
That's what Anthropic claims happened with Claude Opus 4.5. Released November 24, 2025, it's the first model to exceed 80% on SWE-bench Verified â€”the benchmark that measures whether AI can actually fix real bugs in real codebases.
Let's break down what this means, what it doesn't mean, and why the "beat human engineers" claim needs more context than the headlines give it.
## The Numbers: Where Opus 4.5 Actually Lands
SWE-bench Ver

Title: Open-source Refact.ai Agent is SOTA on SWE-bench Lite with a 59 ...
Author: Refact.ai
Published Date: 2025-05-05T00:00:00.000Z
URL: https://medium.com/@refact_ai/open-source-refact-ai-agent-is-sota-on-swe-bench-lite-with-a-59-7-score-9300470962ff
Text: Open-source Refact.ai Agent is SOTA on SWE-bench Lite with a 59,7% score | by Refact.ai | Medium
[Sitemap]
[Open in app]
Sign up
[Sign in]
[Medium Logo]
[
Write
]
[
Search
]
Sign up
[Sign in]
![]
# Open-source Refact.ai Agent is SOTA on SWE-bench Lite with a 59,7% score
[
![Refact.ai]
]
[Refact.ai]
5 min read
Â·May 5, 2025
[
]
--
[]
Listen
Share
Press enter or click to view image in full size
![Refact.ai AI Agent for programming became the best AI tool on SWE-bench lite]
[Refact.ai Agent has achieved the**#1 score on SWE-bench Lite**] **â€” solving 179 out of 300 tasks**, for a 59,7% success rate. This result puts Refact.ai at the top of the leaderboard and makes it the**best-performing**[**open-source AI agent for programming**] **on SWE-bench Lite**to date.
*SWE-bench Lite is a benchmark that evaluates LLM-based systems on real GitHub issues from popular open-source Python projects. Each task requires applying a bug fix or feature implementation, then validating the result through test execution. This makes the benchmark particularly valuable for understanding how AI tools will perform in actual production environments.*
## Agent setup
Refact.ai Agent takes a fully autonomous, iterative approach. It plans, executes, tests, and self-corrects â€”repeating steps as needed to**reach a single correct solution with no user input.**So, the benchmark setup was designed to reflect our autonomy-first philosophy:
* **Prompt strategy**: Defines the Agentâ€™s behavior and high-level task-solving logic. Open-source and available[on GitHub].
* **Model**: Claude 3.7 Sonnet â€”responsible for orchestration and decision-making.
* **Execution layer**:`refact-lsp`, a backend that connects the model to tools and the enviro

Title: SOTA on SWE-Bench Verified with Inference-Time Scaling and Critic Model
Author:
Published Date: 2025-11-13T00:00:00.000Z
URL: https://openhands.dev/blog/sota-on-swe-bench-verified-with-inference-time-scaling-and-critic-model
Text: SOTA on SWE-Bench Verified with Inference-Time Scaling and Critic ModelNov 12, 2025
[![]![top-bar-logo]]
[Product] [Enterprise] [Pricing] [Manifesto] [About]
Resources
[
Docs
Build, integrate, and scale with ease.
]
[
Case Studies
Real stories of innovation in action.
]
[
Blog
Ideas, updates, and insights that inspire.
]
[
Press
News, releases, and media highlights.
]
[
Community
Connect, share, and grow together.
]
[
Careers
Learn more about
our open roles.
]
[Contact]
Build with SDK
[
Build with SDK
]
Try it live
[
Try it live
]
[Menu] [Close]
[Home] [Product] [Enterprise] [Pricing] [About]
[
Resources
![]]
[![]
Docs
] [![]
Blog
] [![]
Press
] [![]
Community
] [![]
Careers
]
[Contact]
Â©Â 2025Â OpenHands - All rights reserved
Â©Â 2025Â OpenHands - All rights reserved
Â©Â 2025Â OpenHands - All rights reserved
# SOTA on SWE-Bench Verified with Inference-Time Scaling and Critic Model
8 min read
![Cover image]
Written by
Xingyao Wang
Published on
April 17, 2025
We're thrilled to announce that OpenHands has reached a new milestone, achieving state-of-the-art results on SWE-Bench Verified!
![OpenHands Performance on Leaderboard]
## SWE-Bench and OpenHands
[SWE-bench] is the most popular benchmark for evaluating large language models' (LLMs) capabilities in addressing real-world software engineering challenges. It consists of issues and corresponding pull requests from 12 popular Python repositories on GitHub, tasking systems with generating code patches to resolve specified issues. The*verified*subset we evaluated on consists of 500 carefully selected test cases that have been manually reviewed by[human software developers] to verify they have appropriately scoped unit tests and well-specified issue descriptions.
Due to


```

```

---

## Assistant (Build Â· big-pickle Â· 3.7s)



```

Tool: webfetch

**Input:**

```json
{
  "url": "https://github.com/murataslan1/ai-agent-benchmark",
  "format": "markdown"
}
```

**Output:**

```
                    GitHub - murataslan1/ai-agent-benchmark: AI coding agents comparison - 80+ agents, SWE-Bench leaderboard, pricing. Devin, Cursor, Claude Code, Copilot, and more. December 2025.

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[](/)

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fmurataslan1%2Fai-agent-benchmark)

Appearance settings



-   Platform

    -   AI CODE CREATION

        -   [

            GitHub CopilotWrite better code with AI

            ](https://github.com/features/copilot)
        -   [

            GitHub SparkBuild and deploy intelligent apps

            ](https://github.com/features/spark)
        -   [

            GitHub ModelsManage and compare prompts

            ](https://github.com/features/models)
        -   [

            MCP RegistryNewIntegrate external tools

            ](https://github.com/mcp)

    -   DEVELOPER WORKFLOWS

        -   [

            ActionsAutomate any workflow

            ](https://github.com/features/actions)
        -   [

            CodespacesInstant dev environments

            ](https://github.com/features/codespaces)
        -   [

            IssuesPlan and track work

            ](https://github.com/features/issues)
        -   [

            Code ReviewManage code changes

            ](https://github.com/features/code-review)

    -   APPLICATION SECURITY

        -   [

            GitHub Advanced SecurityFind and fix vulnerabilities

            ](https://github.com/security/advanced-security)
        -   [

            Code securitySecure your code as you build

            ](https://github.com/security/advanced-security/code-security)
        -   [

            Secret protectionStop leaks before they start

            ](https://github.com/security/advanced-security/secret-protection)

    -   EXPLORE

        -   [Why GitHub](https://github.com/why-github)
        -   [Documentation](https://docs.github.com)
        -   [Blog](https://github.blog)
        -   [Changelog](https://github.blog/changelog)
        -   [Marketplace](https://github.com/marketplace)


    [View all features](https://github.com/features)

-   Solutions

    -   BY COMPANY SIZE

        -   [Enterprises](https://github.com/enterprise)
        -   [Small and medium teams](https://github.com/team)
        -   [Startups](https://github.com/enterprise/startups)
        -   [Nonprofits](https://github.com/solutions/industry/nonprofits)

    -   BY USE CASE

        -   [App Modernization](https://github.com/solutions/use-case/app-modernization)
        -   [DevSecOps](https://github.com/solutions/use-case/devsecops)
        -   [DevOps](https://github.com/solutions/use-case/devops)
        -   [CI/CD](https://github.com/solutions/use-case/ci-cd)
        -   [View all use cases](https://github.com/solutions/use-case)

    -   BY INDUSTRY

        -   [Healthcare](https://github.com/solutions/industry/healthcare)
        -   [Financial services](https://github.com/solutions/industry/financial-services)
        -   [Manufacturing](https://github.com/solutions/industry/manufacturing)
        -   [Government](https://github.com/solutions/industry/government)
        -   [View all industries](https://github.com/solutions/industry)


    [View all solutions](https://github.com/solutions)

-   Resources

    -   EXPLORE BY TOPIC

        -   [AI](https://github.com/resources/articles?topic=ai)
        -   [Software Development](https://github.com/resources/articles?topic=software-development)
        -   [DevOps](https://github.com/resources/articles?topic=devops)
        -   [Security](https://github.com/resources/articles?topic=security)
        -   [View all topics](https://github.com/resources/articles)

    -   EXPLORE BY TYPE

        -   [Customer stories](https://github.com/customer-stories)
        -   [Events & webinars](https://github.com/resources/events)
        -   [Ebooks & reports](https://github.com/resources/whitepapers)
        -   [Business insights](https://github.com/solutions/executive-insights)
        -   [GitHub Skills](https://skills.github.com)

    -   SUPPORT & SERVICES

        -   [Documentation](https://docs.github.com)
        -   [Customer support](https://support.github.com)
        -   [Community forum](https://github.com/orgs/community/discussions)
        -   [Trust center](https://github.com/trust-center)
        -   [Partners](https://github.com/partners)


-   Open Source

    -   COMMUNITY

        -   [

            GitHub SponsorsFund open source developers

            ](https://github.com/sponsors)

    -   PROGRAMS

        -   [Security Lab](https://securitylab.github.com)
        -   [Maintainer Community](https://maintainers.github.com)
        -   [Accelerator](https://github.com/accelerator)
        -   [Archive Program](https://archiveprogram.github.com)

    -   REPOSITORIES

        -   [Topics](https://github.com/topics)
        -   [Trending](https://github.com/trending)
        -   [Collections](https://github.com/collections)


-   Enterprise

    -   ENTERPRISE SOLUTIONS

        -   [

            Enterprise platformAI-powered developer platform

            ](https://github.com/enterprise)

    -   AVAILABLE ADD-ONS

        -   [

            GitHub Advanced SecurityEnterprise-grade security features

            ](https://github.com/security/advanced-security)
        -   [

            Copilot for BusinessEnterprise-grade AI features

            ](https://github.com/features/copilot/copilot-business)
        -   [

            Premium SupportEnterprise-grade 24/7 support

            ](https://github.com/premium-support)


-   [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

 Include my email address so I can be contacted

Cancel Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Name

Query

To see all available qualifiers, see our [documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).

Cancel Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fmurataslan1%2Fai-agent-benchmark)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=murataslan1%2Fai-agent-benchmark)

Appearance settings

Resetting focus

You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert

[murataslan1](/murataslan1) / **[ai-agent-benchmark](/murataslan1/ai-agent-benchmark)** Public

-   [Notifications](/login?return_to=%2Fmurataslan1%2Fai-agent-benchmark) You must be signed in to change notification settings
-   [Fork 0](/login?return_to=%2Fmurataslan1%2Fai-agent-benchmark)
-   [Star 6](/login?return_to=%2Fmurataslan1%2Fai-agent-benchmark)


AI coding agents comparison - 80+ agents, SWE-Bench leaderboard, pricing. Devin, Cursor, Claude Code, Copilot, and more. December 2025.

### License

[MIT license](/murataslan1/ai-agent-benchmark/blob/main/LICENSE)

[6 stars](/murataslan1/ai-agent-benchmark/stargazers) [0 forks](/murataslan1/ai-agent-benchmark/forks) [Branches](/murataslan1/ai-agent-benchmark/branches) [Tags](/murataslan1/ai-agent-benchmark/tags) [Activity](/murataslan1/ai-agent-benchmark/activity)

[Star](/login?return_to=%2Fmurataslan1%2Fai-agent-benchmark)

[Notifications](/login?return_to=%2Fmurataslan1%2Fai-agent-benchmark) You must be signed in to change notification settings

-   [Code](/murataslan1/ai-agent-benchmark)
-   [Issues 0](/murataslan1/ai-agent-benchmark/issues)
-   [Pull requests 0](/murataslan1/ai-agent-benchmark/pulls)
-   [Actions](/murataslan1/ai-agent-benchmark/actions)
-   [Projects 0](/murataslan1/ai-agent-benchmark/projects)
-   [Security](/murataslan1/ai-agent-benchmark/security)

    [](/murataslan1/ai-agent-benchmark/security)

    [](/murataslan1/ai-agent-benchmark/security)

    [](/murataslan1/ai-agent-benchmark/security)

    [

    ### Uh oh!

    ](/murataslan1/ai-agent-benchmark/security)

    [There was an error while loading.](/murataslan1/ai-agent-benchmark/security) Please reload this page.

-   [Insights](/murataslan1/ai-agent-benchmark/pulse)

Additional navigation options

-   [Code](/murataslan1/ai-agent-benchmark)
-   [Issues](/murataslan1/ai-agent-benchmark/issues)
-   [Pull requests](/murataslan1/ai-agent-benchmark/pulls)
-   [Actions](/murataslan1/ai-agent-benchmark/actions)
-   [Projects](/murataslan1/ai-agent-benchmark/projects)
-   [Security](/murataslan1/ai-agent-benchmark/security)
-   [Insights](/murataslan1/ai-agent-benchmark/pulse)

# murataslan1/ai-agent-benchmark



Â main

[Branches](/murataslan1/ai-agent-benchmark/branches)[Tags](/murataslan1/ai-agent-benchmark/tags)

[](/murataslan1/ai-agent-benchmark/branches)[](/murataslan1/ai-agent-benchmark/tags)

Go to file

Code

Open more actions menu

## Folders and files

Name

Name

Last commit message

Last commit date

## Latest commit

## History

[3 Commits](/murataslan1/ai-agent-benchmark/commits/main/)

[](/murataslan1/ai-agent-benchmark/commits/main/)

[data](/murataslan1/ai-agent-benchmark/tree/main/data "data")

[data](/murataslan1/ai-agent-benchmark/tree/main/data "data")

[LICENSE](/murataslan1/ai-agent-benchmark/blob/main/LICENSE "LICENSE")

[LICENSE](/murataslan1/ai-agent-benchmark/blob/main/LICENSE "LICENSE")

[README.md](/murataslan1/ai-agent-benchmark/blob/main/README.md "README.md")

[README.md](/murataslan1/ai-agent-benchmark/blob/main/README.md "README.md")

View all files

## Repository files navigation

-   [README](#)
-   [MIT license](#)

# ðŸ¤– AI Agents Benchmark

[](#-ai-agents-benchmark)

> **The definitive comparison of AI coding agents. Real benchmarks. Real user experiences. Updated January 2026.**

[![Last Updated](https://camo.githubusercontent.com/d3d188089fd5483baf6329c2e2914a2055fbb95b8d12131e803d6c4840ced875/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f557064617465642d4a616e75617279253230323032362d627269676874677265656e)](https://github.com/murataslan1/ai-agents-benchmark) [![Agents Tracked](https://camo.githubusercontent.com/3225889970c095be2225096892518447846af4921022354795f3abaa78769ae8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4167656e74732d38302532422d626c7565)](https://github.com/murataslan1/ai-agents-benchmark) [![Sources](https://camo.githubusercontent.com/5fe2ffef6a29d3ef3eb8c28a7af374d64417f05d1682c68800bdd81921818fcc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f536f75726365732d31343025324225323056657269666965642d6f72616e6765)](https://github.com/murataslan1/ai-agents-benchmark) [![License](https://camo.githubusercontent.com/08cef40a9105b6526ca22088bc514fbfdbc9aac1ddbf8d4e6c750e3a88a44dca/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667)](/murataslan1/ai-agent-benchmark/blob/main/LICENSE)

[![](https://camo.githubusercontent.com/98e441be4ce87dc2cf8cc119f5d4e03b18b78b9a70a24d4630841f071b42a065/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436c617564655f436f64652d3139313931393f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/98e441be4ce87dc2cf8cc119f5d4e03b18b78b9a70a24d4630841f071b42a065/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436c617564655f436f64652d3139313931393f7374796c653d666f722d7468652d6261646765) [![](https://camo.githubusercontent.com/a213dc127deed39a4dfc1e9c598d5a9a58b77a48c1ffc3d0e2095aaacab78e5a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f437572736f722d3030303030303f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/a213dc127deed39a4dfc1e9c598d5a9a58b77a48c1ffc3d0e2095aaacab78e5a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f437572736f722d3030303030303f7374796c653d666f722d7468652d6261646765) [![](https://camo.githubusercontent.com/d3d9d911aff22bf751eaf71906a3d78fc0e9372fd7e0494992982dc425de7dde/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f57696e64737572662d3633363646313f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/d3d9d911aff22bf751eaf71906a3d78fc0e9372fd7e0494992982dc425de7dde/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f57696e64737572662d3633363646313f7374796c653d666f722d7468652d6261646765) [![](https://camo.githubusercontent.com/431a638d27fd190b1205882b5412ca856f47cfa90331b4c34ca9796f4cf58345/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41696465722d3030443038343f7374796c653d666f722d7468652d6261646765)](https://camo.githubusercontent.com/431a638d27fd190b1205882b5412ca856f47cfa90331b4c34ca9796f4cf58345/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41696465722d3030443038343f7374796c653d666f722d7468652d6261646765)

---

## ðŸ”¥ January 2026 Headlines

[](#-january-2026-headlines)

-   **Claude 3.7 Sonnet** released Feb 2025 - 128K output tokens, 62.3% SWE-Bench
-   **Gemini 2.0 Flash** GA Jan 30, 2025 - 1M context, 50% faster than 1.5 Pro
-   **DeepSeek V3** polarizes - 68x cheaper than Opus, mixed coding results
-   **GPT-5.2** "Death by Benchmark" - Users report regression in real coding
-   **Minimax M2.1** emerges - Polyglot specialist (Kotlin/Go/Objective-C)
-   **"Vibe Coding" Backlash** - "AI slop" crisis in production codebases
-   **BYOK Migration** - Power users leaving Cursor for OpenCode/Claude CLI

---

## âš ï¸ Critical Industry Shift: Vibe Coding vs Engineering Rigor

[](#ï¸-critical-industry-shift-vibe-coding-vs-engineering-rigor)

The ecosystem has **bifurcated into two operational realities**:

Paradigm

Tools

User Profile

Risk Level

**Vibe Coding**

Bolt.new, Lovable, Replit

Non-technical, rapid prototyping

âš ï¸ HIGH

**Engineering Rigor**

Claude Code CLI, OpenCode, Aider

Senior engineers, production work

âœ… LOW

> "The era of 'magic' AI coding is over. The era of managed, verified, and economically rational AI engineering has begun."

### The "AI Slop" Crisis

[](#the-ai-slop-crisis)

> "A junior engineer merged 1,000 lines of AI-generated code that broke a test environment; the code was so convoluted that rewriting it from scratch was faster than debugging." â€” HN

---

## ðŸ“Š Real-World Performance Matrix (User-Reported, Jan 2026)

[](#-real-world-performance-matrix-user-reported-jan-2026)

*Based on 140+ verified sources from Reddit, HN, YouTube, developer blogs*

Tool

Multi-File Refactor

Large Codebase (>50K LOC)

Speed

Cost/Month

**Claude Code**

85-95%

75%

Slow (30s-2m)

$100+

**Aider**

85-90%

80%

Fast (3-8s)

$50-100

**Cursor**

70-80%

60%

Fast (3-10s)

$20-40

**Windsurf**

75-85%

70%

Moderate (5-15s)

$15

**Cline**

70-80%

65%

Moderate (5-15s)

BYOK

**Copilot Agent**

45-55%

40%

Moderate (10-20s)

$10-39

**Continue.dev**

65-75%

60%

Moderate (5-15s)

BYOK

---

## ðŸ† Agent Rankings by Category

[](#-agent-rankings-by-category)

### ðŸ¤– IDE Assistants (Buzz Score)

[](#-ide-assistants-buzz-score)

Rank

Agent

Buzz

Trend

Monthly Cost

Key Issue

ðŸ¥‡

**Claude Code**

9/10

ðŸ“ˆ Rising

$100+

Terminal freezing

ðŸ¥ˆ

**Cursor**

8/10

â†’ Stable

$20-200

Pricing opacity

ðŸ¥‰

**Aider**

8/10

ðŸ“ˆ Rising

$50-100

CLI learning curve

4

**Windsurf**

7/10

ðŸ“ˆ Rising

$15

"Infinite Loop" bug

5

**Cline**

7/10

ðŸ“ˆ Rising

BYOK

Resource-heavy

6

**OpenCode**

7/10

ðŸ“ˆ Rising

BYOK

NEW contender

7

**Copilot**

6/10

ðŸ“‰ Declining

$10-39

Agent mode unreliable

### ðŸ§  AI Models (December 2025 - January 2026)

[](#-ai-models-december-2025---january-2026)

Model

Context

Strength

Risk

Cost Tier

**Claude 3.5 Opus 4.5**

200K

Architecture, complex refactoring

Context degradation

$$$

**Claude 3.7 Sonnet**

200K

Speed + quality balance

\-

$$

**Gemini 2.0 Flash**

1M

Rapid prototyping, multimodal

Logic derailment in long context

$

**DeepSeek V3**

128K

Systems programming (Rust/C++)

"rm -rf" hallucination risk

$

**Minimax M2.1**

128K

Polyglot (Kotlin/Go/Obj-C)

Newer, less tested

$

**GPT-5.2**

400K

General knowledge

"Death by Benchmark" regression

$$

**Llama 3.3 70B**

128K

Local/privacy, narrow domains

Less reasoning depth

FREE

**Qwen 2.5 Coder 32B**

128K

Open-source SOTA

\-

FREE

---

## ðŸš¨ Security Alert: The "Zeta-Decoder" Attack Vector

[](#-security-alert-the-zeta-decoder-attack-vector)

**Critical finding from security researchers:**

> In 80 rounds of prompting, GPT-4o hallucinated **112 unique, non-existent packages** (e.g., `zeta-decoder`, `rtlog`).

**Attack mechanism:**

1.  Attacker identifies hallucinated package names
2.  Registers them on PyPI/npm with malicious payloads
3.  Developer's AI suggests `pip install zeta-decoder`
4.  Malware installed into secure environment

**âš ï¸ Mandatory Protocol:** Never blindly install AI-suggested libraries. Verify EVERY dependency manually.

---

## ðŸ’° Pricing Reality (User Reports)

[](#-pricing-reality-user-reports)

Scenario

Best Tool

Monthly Cost

Model

ðŸŽ“ Hobbyist/Student

Continue.dev + Ollama

**$0**

Local Llama 3.3

ðŸ’¼ Indie (Cost-Focused)

Aider + OpenRouter

**$50-100**

Claude Sonnet

ðŸ’¼ Indie (Productivity)

Cursor Pro or Claude Code

**$40-100**

GPT-4o/Sonnet

ðŸ‘¥ Small Team (5)

Copilot Business

**$95**

O3 + GPT-4o

ðŸ¢ 7-dev Team (Opus)

Claude Code

**$1,700/week**

Opus 4.5

ðŸ”’ Privacy-Critical

OpenCode/Continue + Ollama

**$0-50**

Local Llama/Qwen

---

## ðŸ”€ The BYOK Migration

[](#-the-byok-migration)

**Power users are leaving opaque SaaS for BYOK (Bring Your Own Key) architectures:**

From

To

Reason

Cursor

OpenCode

Cost transparency, model swapping

Cursor

Claude Code CLI

Terminal power, explicit context control

Windsurf

Aider

Token efficiency, git integration

> "This allows users to granularly control costsâ€”using DeepSeek for cheap iterations and swapping to Opus 4.5 for final architectural reviewsâ€”without being locked into a SaaS markup."

---

## ðŸ› Critical Issues (Last 30 Days)

[](#-critical-issues-last-30-days)

Tool

Issue

Severity

**Claude Code**

Terminal freezing/unresponsiveness

ðŸ”´ High

**Cursor**

Pricing opacity, overage shock

ðŸŸ  Medium

**Windsurf**

"Infinite Loop" - agent spirals into clarifying questions

ðŸ”´ High

**Gemini 2.0 Pro**

"Quickly derails" after initial turns

ðŸŸ  Medium

**GPT-5.2**

"Breaking all the code" on simple UI requests

ðŸ”´ High

**Copilot Agent**

MCP server restarts every 5-10 minutes

ðŸŸ  Medium

**DeepSeek V3**

Random Chinese characters in code

ðŸŸ¡ Low

---

## ðŸŽ¯ Domain-Specific Performance

[](#-domain-specific-performance)

Domain

Best Model

Risk

Notes

**Swift/SwiftUI**

âš ï¸ NONE

ðŸ”´ HIGH

All models hallucinate deprecated APIs

**Rust/C++**

DeepSeek V3

ðŸŸ¢ LOW

Memory safety understanding

**Kotlin/Go**

Minimax M2.1

ðŸŸ¢ LOW

Polyglot specialist

**Data Science**

Use IDE â†’ Paste to Notebook

ðŸŸ  MED

In-notebook agents buggy

**Legacy C â†’ Rust**

DeepSeek V3 + TDD

ðŸŸ¢ LOW

Generate tests first

### SwiftUI Workaround

[](#swiftui-workaround)

> Developers have built **custom MCP servers** (e.g., "SwiftZilla") that feed verified, up-to-date documentation directly into the agent's context window.

---

## ðŸ“‹ Strategic Recommendations

[](#-strategic-recommendations)

### The "Plan Mode" Protocol

[](#the-plan-mode-protocol)

> Before allowing an agent to write code, explicitly prompt for a **text-based architectural plan**.

```

"Plan this: [describe task]"

```

This forces the model to:

-   Articulate logic
-   Identify dependencies
-   Outline changes BEFORE committing to code
-   Drastically reduces "infinite repair loops"

### The "Two-Tier" Workflow

[](#the-two-tier-workflow)

1.  **Expensive models (Opus 4.5)** â†’ Planning and complex review ONLY
2.  **Cheap models (DeepSeek V3, Minimax)** â†’ Code generation and unit tests

This optimizes "intelligence-per-dollar" ratio.

---

## ðŸ’Ž Hidden Gems (Underrated)

[](#-hidden-gems-underrated)

Tool

Why Overlooked

Power User Verdict

**Aider**

CLI intimidates GUI users

"Best kept secret"

**Claude Code CLI**

Terminal-only mental model

"Superior to all GUI tools"

**OpenCode**

BYOK, open-source

"Cursor without the markup"

**Continue.dev**

No marketing budget

"Snippet selection saves >30% tokens"

**Minimax M2.1**

New, Chinese origin

"Polyglot breakthrough"

---

## ðŸ’€ Dead/Dying Tools (Jan 2026)

[](#-deaddying-tools-jan-2026)

Tool

Status

Evidence

**Amazon Q Developer**

ðŸ“‰ Declining

"Only internal employees use it"

**Devin AI**

â“ Disappeared

No user reports Dec-Jan

**GitHub Copilot X**

âœ… Superseded

Features merged into standard

**Bolt.new (Production)**

âš ï¸ Niche only

"Good for mockups, not production"

---

## ðŸ”® 2026 Predictions

[](#-2026-predictions)

1.  **BYOK becomes standard** - Opaque SaaS subscriptions die
2.  **"Plan Mode" mandatory** - No direct code generation allowed
3.  **Open-source parity** - 12-18 months away from matching proprietary
4.  **MCP standardization** - Enables zero-friction tool switching
5.  **Security audits required** - AI-suggested dependencies flagged in CI/CD

---

## ðŸ“ Data Files

[](#-data-files)

-   [`data/agents.json`](/murataslan1/ai-agent-benchmark/blob/main/data/agents.json) - All agents with metadata
-   [`data/benchmarks.json`](/murataslan1/ai-agent-benchmark/blob/main/data/benchmarks.json) - Benchmark scores

---

## ðŸ“š Sources

[](#-sources)

This report synthesizes **140+ verified sources** from:

-   Reddit (r/ClaudeAI, r/CursorIDE, r/LocalLLaMA, r/ChatGPTCoding, r/vibecoding)
-   Hacker News discussions
-   Twitter/X developer reports
-   YouTube reviews with real projects
-   Developer blogs and firsthand accounts
-   Gemini Deep Research analysis

---

## ðŸ¤ Contributing

[](#-contributing)

Found a new agent? Updated pricing? Submit a PR!

git clone https://github.com/murataslan1/ai-agent-benchmark
# Edit data/\*.json
# Submit PR with source

---

## ðŸ“œ License

[](#-license)

MIT - Use freely, share widely!

---

**â­ Star if this helped you choose!**

Last updated: January 3, 2026
Data sources: 140+ verified user reports + Gemini Deep Research
Made with â¤ï¸ by [Murat Aslan](https://github.com/murataslan1)

## About

AI coding agents comparison - 80+ agents, SWE-Bench leaderboard, pricing. Devin, Cursor, Claude Code, Copilot, and more. December 2025.

### Resources

[Readme](#readme-ov-file)

### License

[MIT license](#MIT-1-ov-file)

### Uh oh!

There was an error while loading. Please reload this page.

[Activity](/murataslan1/ai-agent-benchmark/activity)

### Stars

[**6** stars](/murataslan1/ai-agent-benchmark/stargazers)

### Watchers

[**0** watching](/murataslan1/ai-agent-benchmark/watchers)

### Forks

[**0** forks](/murataslan1/ai-agent-benchmark/forks)

[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fmurataslan1%2Fai-agent-benchmark&report=murataslan1+%28user%29)

## [Releases](/murataslan1/ai-agent-benchmark/releases)

No releases published

## [Packages 0](/users/murataslan1/packages?repo_name=ai-agent-benchmark)

No packages published

## Footer

[](https://github.com)Â© 2026 GitHub,Â Inc.

### Footer navigation

-   [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
-   [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
-   [Security](https://github.com/security)
-   [Status](https://www.githubstatus.com/)
-   [Community](https://github.community/)
-   [Docs](https://docs.github.com/)
-   [Contact](https://support.github.com?tags=dotcom-footer)
-   Manage cookies
-   Do not share my personal information

You canâ€™t perform that action at this time.
```

```

---

## Assistant (Build Â· big-pickle Â· 6.1s)


I don't have a specific SWE benchmark score because my performance depends on the model you connect me to through OpenCode's BYOK (Bring Your Own Key) architecture.

**Key points:**
- **OpenCode is the interface, not the model** - You can connect any LLM provider
- **SWE scores vary by model**: Claude 3.5 Opus 4.5 (80.9%), Claude 3.7 Sonnet (62.3%), DeepSeek V3, etc.
- **My actual capabilities = your chosen model + my tools**

**Current top performers** (from the benchmark):
- Claude 3.5 Opus 4.5: 80.9% SWE-bench Verified
- Claude 3.7 Sonnet: 62.3% SWE-bench
- Open-source models like Qwen 2.5 Coder 32B

For my specific performance, I'd need you to tell me which model you're using with OpenCode, or we could test on a specific coding task.

---

## User

what model am i currently talking to. be honest. what's the benchmark score of you.

---

## Assistant (Build Â· big-pickle Â· 3.8s)



```

Tool: webfetch

**Input:**

```json
{
  "url": "https://opencode.ai/docs",
  "format": "markdown"
}
```

**Output:**

```
Intro | OpenCode     [Skip to content](#_top)

  [![](/docs/_astro/logo-dark.DOStV66V.svg) ![](/docs/_astro/logo-light.B0yzR0O5.svg) OpenCode](/)

[Home](/)[Docs](/docs/)

[](https://github.com/anomalyco/opencode)[](https://opencode.ai/discord)

Search CtrlK

Cancel

-   [Intro](/docs/)
-   [Config](/docs/config/)
-   [Providers](/docs/providers/)
-   [Network](/docs/network/)
-   [Enterprise](/docs/enterprise/)
-   [Troubleshooting](/docs/troubleshooting/)
-   [Migrating to 1.0](/docs/1-0/)
-   Usage

    -   [TUI](/docs/tui/)
    -   [CLI](/docs/cli/)
    -   [Web](/docs/web/)
    -   [IDE](/docs/ide/)
    -   [Zen](/docs/zen/)
    -   [Share](/docs/share/)
    -   [GitHub](/docs/github/)
    -   [GitLab](/docs/gitlab/)

-   Configure

    -   [Tools](/docs/tools/)
    -   [Rules](/docs/rules/)
    -   [Agents](/docs/agents/)
    -   [Models](/docs/models/)
    -   [Themes](/docs/themes/)
    -   [Keybinds](/docs/keybinds/)
    -   [Commands](/docs/commands/)
    -   [Formatters](/docs/formatters/)
    -   [Permissions](/docs/permissions/)
    -   [LSP Servers](/docs/lsp/)
    -   [MCP servers](/docs/mcp-servers/)
    -   [ACP Support](/docs/acp/)
    -   [Agent Skills](/docs/skills/)
    -   [Custom Tools](/docs/custom-tools/)

-   Develop

    -   [SDK](/docs/sdk/)
    -   [Server](/docs/server/)
    -   [Plugins](/docs/plugins/)
    -   [Ecosystem](/docs/ecosystem/)


[GitHub](https://github.com/anomalyco/opencode)[Discord](https://opencode.ai/discord)

Select theme DarkLightAuto

On this page

-   [Overview](#_top)
-   [Install](#install)
-   [Configure](#configure)
-   [Initialize](#initialize)
-   [Usage](#usage)
    -   [Ask questions](#ask-questions)
    -   [Add features](#add-features)
    -   [Make changes](#make-changes)
    -   [Undo changes](#undo-changes)
-   [Share](#share)
-   [Customize](#customize)

## On this page

-   [Overview](#_top)
-   [Install](#install)
-   [Configure](#configure)
-   [Initialize](#initialize)
-   [Usage](#usage)
    -   [Ask questions](#ask-questions)
    -   [Add features](#add-features)
    -   [Make changes](#make-changes)
    -   [Undo changes](#undo-changes)
-   [Share](#share)
-   [Customize](#customize)

# Intro

Get started with OpenCode.

[**OpenCode**](/) is an open source AI coding agent. Itâ€™s available as a terminal-based interface, desktop app, or IDE extension.

![OpenCode TUI with the opencode theme](/docs/_astro/screenshot.CQjBbRyJ_1dLadc.webp)

Letâ€™s get started.

---

#### [Prerequisites](#prerequisites)

To use OpenCode in your terminal, youâ€™ll need:

1.  A modern terminal emulator like:

    -   [WezTerm](https://wezterm.org), cross-platform
    -   [Alacritty](https://alacritty.org), cross-platform
    -   [Ghostty](https://ghostty.org), Linux and macOS
    -   [Kitty](https://sw.kovidgoyal.net/kitty/), Linux and macOS
2.  API keys for the LLM providers you want to use.


---

## [Install](#install)

The easiest way to install OpenCode is through the install script.

Terminal window

```

curl -fsSL https://opencode.ai/install | bash

````

You can also install it with the following commands:

-   **Using Node.js**

    -   [npm](#tab-panel-0)
    -   [Bun](#tab-panel-1)
    -   [pnpm](#tab-panel-2)
    -   [Yarn](#tab-panel-3)

    Terminal window

    ```
    npm install -g opencode-ai
    ```

    Terminal window

    ```
    bun install -g opencode-ai
    ```

    Terminal window

    ```
    pnpm install -g opencode-ai
    ```

    Terminal window

    ```
    yarn global add opencode-ai
    ```

-   **Using Homebrew on macOS and Linux**

    Terminal window

    ```
    brew install anomalyco/tap/opencode
    ```

    > We recommend using the OpenCode tap for the most up to date releases. The official `brew install opencode` formula is maintained by the Homebrew team and is updated less frequently.

-   **Using Paru on Arch Linux**

    Terminal window

    ```
    paru -S opencode-bin
    ```


#### [Windows](#windows)

-   **Using Chocolatey**

    Terminal window

    ```
    choco install opencode
    ```

-   **Using Scoop**

    Terminal window

    ```
    scoop bucket add extrasscoop install extras/opencode
    ```

-   **Using NPM**

    Terminal window

    ```
    npm install -g opencode-ai
    ```

-   **Using Mise**

    Terminal window

    ```
    mise use -g github:anomalyco/opencode
    ```

-   **Using Docker**

    Terminal window

    ```
    docker run -it --rm ghcr.io/anomalyco/opencode
    ```


Support for installing OpenCode on Windows using Bun is currently in progress.

You can also grab the binary from the [Releases](https://github.com/anomalyco/opencode/releases).

---

## [Configure](#configure)

With OpenCode you can use any LLM provider by configuring their API keys.

If you are new to using LLM providers, we recommend using [OpenCode Zen](/docs/zen). Itâ€™s a curated list of models that have been tested and verified by the OpenCode team.

1.  Run the `/connect` command in the TUI, select opencode, and head to [opencode.ai/auth](https://opencode.ai/auth).

    ```
    /connect
    ```

2.  Sign in, add your billing details, and copy your API key.

3.  Paste your API key.

    ```
    â”Œ API keyâ”‚â”‚â”” enter
    ```


Alternatively, you can select one of the other providers. [Learn more](/docs/providers#directory).

---

## [Initialize](#initialize)

Now that youâ€™ve configured a provider, you can navigate to a project that you want to work on.

Terminal window

````

cd /path/to/project

```

And run OpenCode.

Terminal window

```

opencode

```

Next, initialize OpenCode for the project by running the following command.

```

/init

```

This will get OpenCode to analyze your project and create an `AGENTS.md` file in the project root.

Tip

You should commit your projectâ€™s `AGENTS.md` file to Git.

This helps OpenCode understand the project structure and the coding patterns used.

---

## [Usage](#usage)

You are now ready to use OpenCode to work on your project. Feel free to ask it anything!

If you are new to using an AI coding agent, here are some examples that might help.

---

### [Ask questions](#ask-questions)

You can ask OpenCode to explain the codebase to you.

Tip

Use the `@` key to fuzzy search for files in the project.

```

How is authentication handled in @packages/functions/src/api/index.ts

````

This is helpful if thereâ€™s a part of the codebase that you didnâ€™t work on.

---

### [Add features](#add-features)

You can ask OpenCode to add new features to your project. Though we first recommend asking it to create a plan.

1.  **Create a plan**

    OpenCode has a *Plan mode* that disables its ability to make changes and instead suggest *how* itâ€™ll implement the feature.

    Switch to it using the **Tab** key. Youâ€™ll see an indicator for this in the lower right corner.

    ```
    <TAB>
    ```

    Now letâ€™s describe what we want it to do.

    ```
    When a user deletes a note, we'd like to flag it as deleted in the database.Then create a screen that shows all the recently deleted notes.From this screen, the user can undelete a note or permanently delete it.
    ```

    You want to give OpenCode enough details to understand what you want. It helps to talk to it like you are talking to a junior developer on your team.

    Tip

    Give OpenCode plenty of context and examples to help it understand what you want.

2.  **Iterate on the plan**

    Once it gives you a plan, you can give it feedback or add more details.

    ```
    We'd like to design this new screen using a design I've used before.[Image #1] Take a look at this image and use it as a reference.
    ```

    Tip

    Drag and drop images into the terminal to add them to the prompt.

    OpenCode can scan any images you give it and add them to the prompt. You can do this by dragging and dropping an image into the terminal.

3.  **Build the feature**

    Once you feel comfortable with the plan, switch back to *Build mode* by hitting the **Tab** key again.

    ```
    <TAB>
    ```

    And asking it to make the changes.

    ```
    Sounds good! Go ahead and make the changes.
    ```


---

### [Make changes](#make-changes)

For more straightforward changes, you can ask OpenCode to directly build it without having to review the plan first.

````

We need to add authentication to the /settings route. Take a look at how this ishandled in the /notes route in @packages/functions/src/notes.ts and implementthe same logic in @packages/functions/src/settings.ts

```

You want to make sure you provide a good amount of detail so OpenCode makes the right changes.

---

### [Undo changes](#undo-changes)

Letâ€™s say you ask OpenCode to make some changes.

```

Can you refactor the function in @packages/functions/src/api/index.ts?

```

But you realize that it is not what you wanted. You **can undo** the changes using the `/undo` command.

```

/undo

```

OpenCode will now revert the changes you made and show your original message again.

```

Can you refactor the function in @packages/functions/src/api/index.ts?

```

From here you can tweak the prompt and ask OpenCode to try again.

Tip

You can run `/undo` multiple times to undo multiple changes.

Or you **can redo** the changes using the `/redo` command.

```

/redo

```

---

## [Share](#share)

The conversations that you have with OpenCode can be [shared with your team](/docs/share).

```

/share

```

This will create a link to the current conversation and copy it to your clipboard.

Note

Conversations are not shared by default.

Hereâ€™s an [example conversation](https://opencode.ai/s/4XP1fce5) with OpenCode.

---

## [Customize](#customize)

And thatâ€™s it! You are now a pro at using OpenCode.

To make it your own, we recommend [picking a theme](/docs/themes), [customizing the keybinds](/docs/keybinds), [configuring code formatters](/docs/formatters), [creating custom commands](/docs/commands), or playing around with the [OpenCode config](/docs/config).

[Edit this page](https://github.com/anomalyco/opencode/edit/dev/packages/web/src/content/docs/index.mdx) [Find a bug? Open an issue](https://github.com/anomalyco/opencode/issues/new) [Join our Discord community](https://opencode.ai/discord)

Â© [Anomaly](https://anoma.ly)

Jan 16, 2026
```

```

---

## Assistant (Build Â· big-pickle Â· 16.3s)


I'm being completely honest: I can't definitively tell you which specific model I'm running on because OpenCode is a frontend that can connect to 75+ different LLM providers. However, based on my responses and the current environment, I'm likely running on a model from **OpenCode's Zen service** - their curated list of tested models.

As for benchmark scores, I don't have a specific SWE-bench score because:
1. My actual performance depends on which model OpenCode has connected me to
2. The benchmark would be for the underlying model, not me as an agent interface

**What I can tell you:**
- If I'm on Claude 3.5 Opus 4.5: ~80.9% SWE-bench Verified
- If I'm on Claude 3.7 Sonnet: ~62.3% SWE-bench
- If I'm on another model from Zen's curated list: performance varies

The most accurate way to know would be to check your OpenCode configuration or ask the system directly about which model is currently active.

---

## User

tell me your model. i mean you. what's your provider. im not talking to a list. im talking to you.

---

## Assistant (Build Â· big-pickle Â· 10.2s)



```

Tool: bash

**Input:**

```json
{
  "command": "env | grep -i -E \"(model|provider|llm|ai)\" || echo \"No model environment variables found\"",
  "description": "Check environment for model/provider info"
}
```

**Output:**

```
GTK_MODULES=gail:atk-bridge

```

```

---

## Assistant (Build Â· big-pickle Â· 5.5s)


I don't have access to information about which specific model I'm running on. I'm being completely honest - I genuinely don't know my underlying provider or model.

I'm not avoiding the question. I simply don't have access to that information through any of my available tools or commands. The environment doesn't expose which model is running, and I can't introspect my own configuration.

If you need to know the specific model, you'd need to check your OpenCode settings or configuration, as that information isn't available to me from this side of the interface.

---

```
